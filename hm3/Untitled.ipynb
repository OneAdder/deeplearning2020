{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imports():\n",
    "    global math, np, pd, random, json, torch, Dataset, DataLoader, tqdm, plt, yttm\n",
    "    \n",
    "    import math\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    import random\n",
    "    import json\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    import youtokentome as yttm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-84ba71c37794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'qa_data.jsonl'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_object\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_object\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mqa_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "qa_data = list()\n",
    "\n",
    "with open('qa_data.jsonl') as file_object:\n",
    "    for line in file_object:\n",
    "        qa_data.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "questin_answer_data = []\n",
    "for question_answer in qa_data:\n",
    "    questin_answer_data.append(question_answer['question'])\n",
    "    deque(map(questin_answer_data.append, question_answer['responses']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['долго ли идут деньги с яндексденег на карту visa?',\n",
       " 'нет. прорыв 35 ;)',\n",
       " 'можно ли зарегистрировать авто в другом регионе',\n",
       " 'можно на родственника из того региона.. .  а потом ездить по доверке',\n",
       " 'что делать если у меня очень тонкие ногти а хочется их отрастить?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questin_answer_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('for_bpe.txt', 'w') as f:\n",
    "    f.write('\\n'.join(questin_answer_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del questin_answer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "долго ли идут деньги с яндексденег на карту visa?\r\n",
      "нет. прорыв 35 ;)\r\n",
      "можно ли зарегистрировать авто в другом регионе\r\n",
      "можно на родственника из того региона.. .  а потом ездить по доверке\r\n",
      "что делать если у меня очень тонкие ногти а хочется их отрастить?\r\n",
      "витамины и умная эмаль (каждый день)\r\n",
      "ванночки с морской солью. с вечера мажь ногти сверху йодом. не бойся, до утра все впитается.\r\n",
      "умная эмаль, витамины, йод, и поменьше крась лаком \r\n",
      "лаки фирмы trind производство usa + кальций\r\n",
      "в чем отличие медитации от йоги?\r\n"
     ]
    }
   ],
   "source": [
    "!head for_bpe.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 30_000\n",
    "MODEL_PATH = 'pretrained_bpe_lm.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<youtokentome.youtokentome.BPE at 0x7f6155470f98>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yttm.BPE.train(data='for_bpe.txt', vocab_size=VOCAB_SIZE, model=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = yttm.BPE(model=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for qa in qa_data:\n",
    "    for answer in qa['responses']:\n",
    "        questions.append(qa['question'])\n",
    "        answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del qa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30341/30341 [01:07<00:00, 448.48it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "tokenized_questions = []\n",
    "\n",
    "for i_batch in tqdm(range(math.ceil(len(questions) / batch_size))):\n",
    "    tokenized_questions.extend(\n",
    "        tokenizer.encode(\n",
    "            list(questions[i_batch*batch_size:(i_batch+1)*batch_size]),\n",
    "            bos=True, eos=False,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# как сложно без gc\n",
    "del questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30341/30341 [01:03<00:00, 480.40it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_answers = []\n",
    "\n",
    "for i_batch in tqdm(range(math.ceil(len(answers) / batch_size))):\n",
    "    tokenized_answers.extend(\n",
    "        tokenizer.encode(\n",
    "            list(answers[i_batch*batch_size:(i_batch+1)*batch_size]),\n",
    "            bos=True, eos=False,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# у меня не хватает памяти, лучше сохраниться\n",
    "import pickle\n",
    "\n",
    "with open('questions', 'wb') as f:\n",
    "    pickle.dump(tokenized_questions, f)\n",
    "\n",
    "with open('answers', 'wb') as f:\n",
    "    pickle.dump(tokenized_answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenized_questions\n",
    "del tokenized_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# будем брать одну десятую датасета, иначе памяти не хватит\n",
    "with open('questions', 'rb') as f:\n",
    "    questions = pickle.load(f)\n",
    "questions = questions[:int(len(questions)/20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers', 'rb') as f:\n",
    "    answers = pickle.load(f)\n",
    "answers = answers[:int(len(answers)/20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388356 388356\n"
     ]
    }
   ],
   "source": [
    "print(len(questions), len(answers))\n",
    "assert len(questions) == len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:          14961        6972        4748         500        3240        7166\r\n",
      "Swap:         16134        2818       13316\r\n"
     ]
    }
   ],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_INDEX = 0\n",
    "EOS_INDEX = 3\n",
    "VOCAB_SIZE = 30_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceBucketingData(torch.utils.data.Dataset):\n",
    "    \"\"\"по сути то же, что в условии, только другие сиквенсы\"\"\"\n",
    "    def __init__(self, questions, answers, max_len, pad_index=PAD_INDEX, eos_index=EOS_INDEX):\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        if len(questions) != len(answers):\n",
    "            raise ValueError('Вопросы и ответы должны быть одной длины')\n",
    "        self.max_len = max_len\n",
    "        self.pad_index = pad_index\n",
    "        self.eos_index = eos_index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def _prepare_sample(self, sequence_q, sequence_a, max_len_q, max_len_a):\n",
    "        sequence_q = sequence_q[:max_len_q]\n",
    "        sequence_a = sequence_a[:max_len_a]\n",
    "        x = sequence_q\n",
    "        y = sequence_a\n",
    "        pads_x = [self.pad_index] * (max_len_q - len(x))\n",
    "        pads_y = [self.pad_index] * (max_len_a - len(y))\n",
    "        x += pads_x\n",
    "        y += pads_y\n",
    "        return x, y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_q = self.questions[index]\n",
    "        batch_a = self.answers[index]\n",
    "        max_len_q = min([\n",
    "            self.max_len,\n",
    "            max(map(len, batch_q)),\n",
    "        ])\n",
    "        max_len_a = min([\n",
    "            self.max_len,\n",
    "            max(map(len, batch_a)),\n",
    "        ])\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for sample_q, sample_a in zip(batch_q, batch_a):\n",
    "            x, y = self._prepare_sample(sample_q, sample_a, self.max_len, self.max_len)\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "        batch_x = torch.tensor(batch_x).long().to(DEVICE)\n",
    "        batch_y = torch.tensor(batch_y).long().to(DEVICE)\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = sorted(questions, key=len)\n",
    "answers = sorted(answers, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем батч побольше\n",
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_q = []\n",
    "batches_a = []\n",
    "\n",
    "for i_batch in range(math.ceil(len(questions) / BATCH_SIZE)):\n",
    "    q = questions[i_batch*BATCH_SIZE:(i_batch+1)*BATCH_SIZE]\n",
    "    a = answers[i_batch*BATCH_SIZE:(i_batch+1)*BATCH_SIZE]\n",
    "    if len(q) != BATCH_SIZE or len(a) != BATCH_SIZE:\n",
    "        continue\n",
    "    batches_q.append(questions[i_batch*BATCH_SIZE:(i_batch+1)*BATCH_SIZE])\n",
    "    batches_a.append(answers[i_batch*BATCH_SIZE:(i_batch+1)*BATCH_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# у нас же нет гарантии, что количество вопросов делится на батч-сайз\n",
    "# мы лишнее убрали условием сверху, надо убедиться\n",
    "all(\n",
    "    map(lambda b: True if len(b) == BATCH_SIZE else False, batches_q)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_start_index = int(len(batches_q) * 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = SequenceBucketingData(\n",
    "    questions=batches_q[:-validation_start_index],\n",
    "    answers=batches_a[:-validation_start_index],\n",
    "    max_len=MAX_LEN)\n",
    "test_seq = SequenceBucketingData(\n",
    "    questions=batches_q[-validation_start_index:],\n",
    "    answers=batches_a[-validation_start_index:],\n",
    "    max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_seq, batch_size=None, batch_sampler=None)\n",
    "validation_loader = torch.utils.data.DataLoader(test_seq, batch_size=None, batch_sampler=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это специальный дропаут для реккуретных сетей\n",
    "# хорошо это объясняется здесь: https://youtu.be/WLaAIYQHHMU?t=1093\n",
    "\n",
    "class SpatialDropout(torch.nn.Dropout2d):\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 300\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сильно переписанный вариант из документации\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        # батч идёт первым, нуже батчфёрст\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x - [batch_size, max_len] \n",
    "        embedded = self.embedding(x)\n",
    "        # embedded - [batch_size, max_len, 300]\n",
    "        _, hidden = self.gru(embedded)\n",
    "        # hidden - [1, batch_size, hidden_size]\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тоже сильно переписанный вариант из документации\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(-1)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # x - [batch_size]\n",
    "        # hidden - [1, batch_size, hidden_size]\n",
    "        x = x.long() # торч не всегда маг с `double` и `long int` в CUDA\n",
    "        embedded = self.embedding(x)\n",
    "        # embedded - [batch_size, hidden_size]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        # embedded - [batch_size, 1, hidden_size]\n",
    "        _, hidden = self.gru(embedded, hidden)\n",
    "        # hidden - [1, batch_size, hidden_size]\n",
    "        predictions = self.out(hidden)\n",
    "        # predictions - [1, batch_size, vocab_size]\n",
    "        predictions = self.softmax(predictions)\n",
    "        # predictions - [1, batch_size, vocab_size]\n",
    "        return predictions, hidden\n",
    "\n",
    "    def init_input(self):\n",
    "        \"\"\"Запускаем декодер с EOS\n",
    "        \n",
    "        Зачем нам BOS, если уже есть EOS, логично?\n",
    "        \"\"\"\n",
    "        input_zeros = torch.zeros(BATCH_SIZE, device=DEVICE)\n",
    "        input_eos = input_zeros + EOS_INDEX\n",
    "        return input_eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, source, target):\n",
    "        outputs = torch.zeros(MAX_LEN, BATCH_SIZE, VOCAB_SIZE)#.to(DEVICE)\n",
    "        # outputs - [max_len, batch_size, vocab_size]\n",
    "        hidden = self.encoder(source)\n",
    "        # hidden - [1, batch_size, hidden_size]\n",
    "        \n",
    "        x = decoder.init_input()\n",
    "        # x - [batch_size]\n",
    "        for t in range(1, MAX_LEN):\n",
    "            predictions, hidden = self.decoder(x, hidden)\n",
    "            prediction = predictions.argmax(-1)\n",
    "            # prediction - [1, batch_size]\n",
    "            prediction = prediction.squeeze(0)\n",
    "            # prediction - [batch_size]\n",
    "            x = prediction\n",
    "            outputs[t] = predictions\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(VOCAB_SIZE, HIDDEN_SIZE).to(DEVICE)\n",
    "decoder = Decoder(HIDDEN_SIZE, VOCAB_SIZE).to(DEVICE)\n",
    "seq2seq = Seq2Seq(encoder, decoder).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(seq2seq.parameters(), lr=LR)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "def train(\n",
    "    train_loader, test_loader, seq2seq,\n",
    "    optimizer, criterion,\n",
    "):\n",
    "    for e in range(1, 10):\n",
    "        print(f'Epoch: {e}')\n",
    "        losses = []\n",
    "        \n",
    "        seq2seq.train()\n",
    "        for x, y in tqdm(train_loader):\n",
    "            # x, y - [batch_size, max_len]\n",
    "            start = perf_counter()\n",
    "            print(0)\n",
    "            output = seq2seq.forward(x, y)\n",
    "            print(perf_counter() - start)\n",
    "            \n",
    "            # для лосса надо смёрджить длину батча и длину последовательности\n",
    "            # output - [max_len, batch_size, vocab_size]\n",
    "            output = output.transpose(0, 1)\n",
    "            output = output.reshape(-1, output.shape[-1]).to(DEVICE)\n",
    "            print(perf_counter() - start)\n",
    "            # output - [batch_size*max_len, vocab_size]\n",
    "            y = y.reshape(-1).to(DEVICE)\n",
    "            print(perf_counter() - start)\n",
    "            # y - [batch_size*max_len]\n",
    "            loss = criterion(output, y)\n",
    "            print(perf_counter() - start)\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(seq2seq.parameters(), max_norm=1)\n",
    "            print(perf_counter() - start)\n",
    "            loss.backward()\n",
    "            print(perf_counter() - start)\n",
    "            optimizer.step()\n",
    "            print(perf_counter() - start)\n",
    "            optimizer.zero_grad()\n",
    "            print(perf_counter() - start)\n",
    "            losses.append(loss.item())\n",
    "            print(perf_counter() - start)\n",
    "        torch.save(seq2seq.state_dict(), f'epoch_{e}_seq2seq.pth')\n",
    "        torch.save(encoder.state_dict(), f'epoch_{e}_encoder.pth')\n",
    "        torch.save(decoder.state_dict(), f'epoch_{e}_decoder.pth')\n",
    "    return losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5765 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/5765 [00:02<3:26:39,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 2/5765 [00:04<3:23:25,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 3/5765 [00:06<3:21:12,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 4/5765 [00:08<3:22:37,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 5/5765 [00:10<3:21:43,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 6/5765 [00:12<3:24:16,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 7/5765 [00:14<3:23:26,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/5765 [00:16<3:51:37,  2.41s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-7bc1a8fa0d79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-7cc22d07ef94>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, test_loader, seq2seq, optimizer, criterion)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_loader, validation_loader, seq2seq, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "del seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "del encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "del decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# почему я не люблю ноутбуки\n",
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# не помогает, чёрт его возми! не весь мусор удаляет\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.2447, 0.6652],\n",
       "        [0.0900, 0.2447, 0.6652],\n",
       "        [0.0900, 0.2447, 0.6652]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2, 3], [1, 2, 3], [1, 2, 3]])\n",
    "torch.softmax(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64])\n",
      "torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.zeros(MAX_LEN, BATCH_SIZE)\n",
    "print(x.shape)\n",
    "x = x.reshape(-1)\n",
    "print(x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
